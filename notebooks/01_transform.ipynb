{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c574b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.float_format', lambda x : '{:,.2f}'.format(x))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b425eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/27 10:12:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "if not ('sc' in locals() or 'sc' in globals()):\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.setMaster('spark://spark-master:7077')\n",
    "    conf.set('spark.executor.memory', '512m')\n",
    "    conf.set('spark.sql.jsonGenerator.ignoreNullFields', 'False') # To dump nules in json\n",
    "    conf.set('spark.app.name', 'basics')\n",
    "\n",
    "    sc = SparkContext.getOrCreate(SparkContext(conf=conf))\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39105625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def validate_fields_cols(df: DataFrame, field_validations: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Validate fields in DataFrame based on specified validations.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        field_validations (list): List of dictionaries containing field validations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the validated DataFrame and a list of error columns.\n",
    "    \"\"\"\n",
    "    error_columns = []\n",
    "    for validation in field_validations:\n",
    "        field = validation['field']\n",
    "        validations = validation['validations']\n",
    "        for v in validations:\n",
    "            error_col_name = f\"{field}_{v}\"  # New column name for error\n",
    "            error_columns.append(error_col_name)\n",
    "            if 'notEmpty' in v:\n",
    "                df = df.withColumn(error_col_name, when(col(field) == \"\", lit(\"KO\")).otherwise(lit(\"OK\")))\n",
    "            if 'notNull' in v:\n",
    "                df = df.withColumn(error_col_name, when(col(field).isNull(), lit(\"KO\")).otherwise(lit(\"OK\")))\n",
    "    return df, error_columns\n",
    "\n",
    "def filter_by_column_values(df: DataFrame, columns: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Filter DataFrame based on column values.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        columns (list): List of column names to filter by.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the filtered DataFrame and a DataFrame containing rows not matching the filter.\n",
    "    \"\"\"\n",
    "    condition = \" AND \".join(f\"{col} = 'OK'\" for col in columns)\n",
    "    filtered_df = df.filter(condition)\n",
    "    not_ok = df.subtract(filtered_df)\n",
    "    return filtered_df.drop(*columns), not_ok\n",
    "\n",
    "def struct_columns_to_single_column(df: DataFrame, column_names: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Combine struct columns into a single column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        column_names (list): List of column names to combine.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with combined struct columns.\n",
    "    \"\"\"\n",
    "    struct_col = F.struct(*[F.col(col_name) for col_name in column_names])\n",
    "    return df.withColumn(\"validations\", struct_col).drop(*column_names)\n",
    "\n",
    "def load_input(spark: SparkSession, **kwargs) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load input data into DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Spark session object.\n",
    "        kwargs: Additional arguments for loading data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    df = spark.read.load(**kwargs)\n",
    "    return df\n",
    "\n",
    "def save_output(df: DataFrame, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Save DataFrame to output location.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to save.\n",
    "        kwargs: Additional arguments for saving data.\n",
    "    \"\"\"\n",
    "    df.write.save(**kwargs)\n",
    "\n",
    "def transform(input_df: DataFrame, transformation_type: str, steps: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Perform transformation on DataFrame.\n",
    "\n",
    "    Args:\n",
    "        input_df (DataFrame): Input DataFrame.\n",
    "        transformation_type (str): Type of transformation to perform.\n",
    "        steps (list): List of transformation steps.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Transformed DataFrame.\n",
    "    \"\"\"\n",
    "    return transform_funcs_dict[transformation_type](input_df, steps)\n",
    "\n",
    "def validate_fields(input_df: DataFrame, steps: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Validate fields in DataFrame.\n",
    "\n",
    "    Args:\n",
    "        input_df (DataFrame): Input DataFrame.\n",
    "        steps (list): List of validation steps.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the DataFrame with valid rows and DataFrame with invalid rows.\n",
    "    \"\"\"\n",
    "    validated_df, val_cols = validate_fields_cols(input_df, steps)\n",
    "    validated_df_OK, validated_df_NOTOK = filter_by_column_values(validated_df, val_cols)\n",
    "    validated_df_NOTOK = struct_columns_to_single_column(validated_df_NOTOK, val_cols)\n",
    "    validated_df_NOTOK = add_current_timestamp(validated_df_NOTOK)\n",
    "    return validated_df_OK, validated_df_NOTOK\n",
    "\n",
    "def add_fields(input_df: DataFrame, params: list) -> list:\n",
    "    \"\"\"\n",
    "    Add new fields to DataFrame based on specified parameters.\n",
    "\n",
    "    Args:\n",
    "        input_df (DataFrame): Input DataFrame.\n",
    "        params (list): List of dictionaries containing parameters for adding fields.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing the modified DataFrame.\n",
    "    \"\"\"\n",
    "    for item in params:\n",
    "        column_name = item['name']\n",
    "        function_name = item['function']\n",
    "        \n",
    "        # Get the function object dynamically\n",
    "        function = globals()[function_name]\n",
    "        \n",
    "        # Apply function and add column to DataFrame\n",
    "        input_df = input_df.withColumn(column_name, function())\n",
    "    return [input_df]\n",
    "\n",
    "def add_current_timestamp(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add current timestamp column to DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with current timestamp column added.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"dt\", current_timestamp())\n",
    "\n",
    "# Dictionary to map transformation types to corresponding functions\n",
    "transform_funcs_dict = dict()\n",
    "transform_funcs_dict[\"validate_fields\"] = validate_fields\n",
    "transform_funcs_dict[\"add_fields\"] = add_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf12058-10c2-44b4-91c8-f60ac18b8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"sources\": [{\n",
    "        \"name\": \"person_inputs\",\n",
    "        \"params\": {\n",
    "            \"path\": \"/opt/workspace/ingestion-metadata-engine/data/input/events/person/*.json\",\n",
    "            \"format\": \"JSON\"\n",
    "        }\n",
    "    }],\n",
    "    \"transformations\": [{\n",
    "            \"name\": \"validation\",\n",
    "            \"type\": \"validate_fields\",\n",
    "            \"params\": {\n",
    "                \"input\": \"person_inputs\",\n",
    "                \"steps\": [\n",
    "                    {\"field\": \"office\", \"validations\": [\"notEmpty\"]},\n",
    "                    {\"field\": \"age\", \"validations\": [\"notNull\"]}\n",
    "                ],\n",
    "                \"outputs\": {\n",
    "                    \"output_ok\": \"validation_ok\",\n",
    "                    \"output_notok\": \"validation_notok\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ok_with_date\",\n",
    "            \"type\": \"add_fields\",\n",
    "            \"params\": {\n",
    "                \"input\": \"validation_ok\",\n",
    "                \"steps\": [\n",
    "                        {\"name\": \"dt\", \"function\": \"current_timestamp\"},\n",
    "                        {\"name\": \"dt2\", \"function\": \"current_timestamp\"}\n",
    "                    ],\n",
    "                \"outputs\": {\n",
    "                    \"output\": \"final_ok\"\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    ,\n",
    "    \"outputs\": [\n",
    "        {\n",
    "            \"input\": \"final_ok\",\n",
    "            \"name\": \"raw-ok\",\n",
    "            \"params\": {\n",
    "                \"path\": \"/opt/workspace/ingestion-metadata-engine/data/output/events/person/raw-ok.json\",\n",
    "                \"format\": \"JSON\",\n",
    "                \"mode\": \"overwrite\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"validation_notok\",\n",
    "            \"name\": \"raw-notok\",\n",
    "            \"params\": {\n",
    "                \"path\": \"/opt/workspace/ingestion-metadata-engine/data/output/discards/person/raw-notok.json\",\n",
    "                \"format\": \"JSON\",\n",
    "                \"mode\": \"overwrite\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"validation_ok\",\n",
    "            \"name\": \"raw-ok\",\n",
    "            \"params\": {\n",
    "                \"path\": \"/opt/workspace/ingestion-metadata-engine/data/output/discards/person/raw-ok.parquet\",\n",
    "                \"format\": \"parquet\",\n",
    "                \"mode\": \"overwrite\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6415f51-ce82-4e92-9d1f-672a8d64f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tasks(spark: SparkSession, sc, metadata_input: dict) -> None:\n",
    "    \"\"\"\n",
    "    Execute tasks specified in the metadata input.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Spark session object.\n",
    "        sc: Spark context.\n",
    "        metadata_input (dict): Metadata input containing sources, transformations, and outputs.\n",
    "    \"\"\"\n",
    "    outputs_dict = {}\n",
    "\n",
    "    # Load input data from sources\n",
    "    for source in metadata_input[\"sources\"]:\n",
    "        outputs_dict[source[\"name\"]] = load_input(spark, **source[\"params\"])\n",
    "\n",
    "    # Perform transformations\n",
    "    for transformation in metadata_input[\"transformations\"]:\n",
    "        transformation_name = transformation[\"name\"]\n",
    "        transformation_type = transformation[\"type\"]\n",
    "        params = transformation[\"params\"]\n",
    "        input_df = params[\"input\"]\n",
    "        steps = params[\"steps\"]\n",
    "        outputs = params[\"outputs\"]\n",
    "    \n",
    "        # Transform data\n",
    "        outs = transform(outputs_dict[input_df], transformation_type, steps)\n",
    "        temp = {value: outs[idx] for idx, value in enumerate(outputs.values())}\n",
    "        outputs_dict = {**temp, **outputs_dict}\n",
    "\n",
    "    # Save output data\n",
    "    for output in metadata_input[\"outputs\"]:\n",
    "        save_output(outputs_dict[output[\"input\"]], **output[\"params\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8040f8-6b7b-4ff2-89ee-23e8a99c3205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "execute_tasks(spark, sc, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c09c7-4a4b-47e1-b8e8-7fac6076f37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
